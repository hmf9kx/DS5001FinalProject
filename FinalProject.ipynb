{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979923b5-0493-4768-ad1a-06db54f0bc7a",
   "metadata": {},
   "source": [
    "# Final Project Notebook\n",
    "\n",
    "DS 5001 Exploratory Text Analytics | Spring 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7046f57f-12ed-4259-be3d-60cb67b8d044",
   "metadata": {},
   "source": [
    "# Metadata\n",
    "\n",
    "- Full Name: Hayden French\n",
    "- Userid: hmf9kx\n",
    "- GitHub Repo URL: https://github.com/hmf9kx/DS5001FinalProject\n",
    "- UVA Box URL: https://virginia.box.com/s/rf7v528sv9goe93qponasve9yyjcjduy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57acd11d-eb04-4bcc-b115-f205f367de49",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The goal of the final project is for you to create a **digital analytical edition** of a corpus using the tools, practices, and perspectives youâ€™ve learning in this course. You will select a corpus that has already been digitized and transcribed, parse that into an F-compliant set of tables, and then generate and visualize the results of a series of fitted models. You will also draw some tentative conclusions regarding the linguistic, cultural, psychological, or historical features represented by your corpus. The point of the exercise is to have you work with a corpus through the entire pipeline from ingestion to interpretation. \n",
    "\n",
    "Specifically, you will acquire a collection of long-form texts and perform the following operations:\n",
    "\n",
    "- **Convert** the collection from their source formats (F0) into a set of tables that conform to the Standard Text Analytic Data Model (F2).\n",
    "- **Annotate** these tables with statistical and linguistic features using NLP libraries such as NLTK (F3).\n",
    "- **Produce** a vector representation of the corpus to generate TFIDF values to add to the TOKEN (aka CORPUS) and VOCAB tables (F4).\n",
    "- **Model** the annotated and vectorized model with tables and features derived from the application of unsupervised methods, including PCA, LDA, and word2vec (F5).\n",
    "- **Explore** your results using statistical and visual methods.\n",
    "- **Present** conclusions about patterns observed in the corpus by means of these operations.\n",
    "\n",
    "When you are finished, you will make the results of your work available in GitHub (for code) and UVA Box (for data). You will submit to Gradescope (via Canvas) a PDF version of a Jupyter notebook that contains the information listed below.\n",
    "\n",
    "# Some Details\n",
    "\n",
    "- Please fill out your answers in each task below by editing the markdown cell. \n",
    "- Replace text that asks you to insert something with the thing, i.e. replace `(INSERT IMAGE HERE)` with an image element, e.g. `![](image.png)`.\n",
    "- For URLs, just paste the raw URL directly into the text area. Don't worry about providing link labels using `[label](link)`.\n",
    "- Please do not alter the structure of the document or cell, i.e. the bulleted lists. \n",
    "- You may add explanatory paragraphs below the bulleted lists.\n",
    "- Please name your tables as they are named in each task below.\n",
    "- Tasks are indicated by headers with point values in parentheses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568b6d68-e039-4612-858b-29510eeb5365",
   "metadata": {},
   "source": [
    "# Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0889de-cd53-4aa5-80b2-a2a39060776a",
   "metadata": {},
   "source": [
    "## Source Description (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9e395a-4b0b-4ba3-9112-80c733998dbe",
   "metadata": {},
   "source": [
    "Provide a brief description of your source material, including its provenance and content. Tell us where you found it and what kind of content it contains.\n",
    "\n",
    "The data for this project come from UC Santa Cruz's Natural Language and Dialogue Systems (NLDS) lab. They have created a publicly available corpus of over a thousand film scripts scraped from the The Internet Movie Script Database (imsdb.com). The scripts are stored as text files and are grouped by the film's genres. I thought it would be interesting to compare movies from different genres via the techniques we have discussed throughout the semester. Note that while the original dataset is very large, I have selected 24 films across 5 genres for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b507c1-6dc2-44f7-b74c-790d84a48e8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Source Features (1)\n",
    "\n",
    "Add values for the following items. (Do this for all following bulleted lists.)\n",
    "\n",
    "- Source URL: https://nlds.soe.ucsc.edu/fc2\n",
    "- UVA Box URL: https://virginia.box.com/s/dl0u1rcar84s5u6btczz956ea3ngwuwf\n",
    "- Number of raw documents: 24\n",
    "- Total size of raw documents (e.g. in MB): 4.45 MB\n",
    "- File format(s), e.g. XML, plaintext, etc.: plaintext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e81b1-9f70-47b5-bb25-49be4e76b98b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Source Document Structure (1)\n",
    "\n",
    "Provide a brief description of the internal structure of each document. That, describe the typical elements found in document and their relation to each other. For example, a corpus of letters might be described as having a date, an addressee, a salutation, a set of content paragraphs, and closing. If they are various structures, state that.\n",
    "\n",
    "Each document consists of the raw text of a movie script scraped from the The Internet Movie Script Database. Documents are generally 100-200 MBs and close to 10,000 lines (including whitespace). The files includes both scene descriptions and the dialogue from the film. Additionally, some scripts included page numbers in the txt file. I manually selected scripts which included page numbers as that added an additional layer to my OHCO structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec4c9f-e101-46fe-ac59-a35a1b148a4b",
   "metadata": {},
   "source": [
    "# Parsed and Annotated Data\n",
    "\n",
    "Parse the raw data into the three core tables of your addition: the `LIB`, `CORPUS`, and `VOCAB` tables.\n",
    "\n",
    "These tables will be stored as CSV files with header rows.\n",
    "\n",
    "You may consider using `|` as a delimitter.\n",
    "\n",
    "Provide the following information for each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d05ce4-ac5c-43ea-a07b-c4626338f80e",
   "metadata": {},
   "source": [
    "## LIB (2)\n",
    "\n",
    "The source documents the corpus comprises. These may be books, plays, newspaper articles, abstracts, blog posts, etc. \n",
    "\n",
    "Note that these are *not* documents in the sense used to describe a bag-of-words representation of a text, e.g. chapter.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/j5y1oa6dr80j0jnh5h9cbovoft15pelh\n",
    "- GitHub URL for notebook used to create: https://github.com/hmf9kx/DS5001FinalProject/blob/main/FinalProjectCode.ipynb\n",
    "- Delimitter: `|`\n",
    "- Number of observations: 24\n",
    "- List of features, including at least three that may be used for model summarization (e.g. date, author, etc.): `script_id`, `name`, `title`, `genre`, `path`, `replace_dict`, `drop_words`, `page_pat`\n",
    "- Average length of each document in characters: ~200,000\n",
    "\n",
    "Notes: While there many scripts in the original data, I selected 4 for each of the 5 genres I was interested in (Western, Sci-Fi, Horror, Comedy, and Romance). This was because parsing the scripts into the OHCO required a lot of manual work. I selected scripts that included page numbers so I could add that level to my hierarchy. The formatting was also not as standardized as I had originally hoped, so I had to specify regexes to match the different page number formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304204a5-00be-46ad-b98b-0d10a9c8ca4b",
   "metadata": {},
   "source": [
    "## CORPUS (2)\n",
    "\n",
    "The sequence of word tokens in the corpus, indexed by their location in the corpus and document structures.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/o0yki6nb7kxp8tjullhxdqs8r4m6bpt0\n",
    "- GitHub URL for notebook used to create: https://github.com/hmf9kx/DS5001FinalProject/blob/main/FinalProjectCode.ipynb\n",
    "- Delimitter: `|`\n",
    "- Number of observations Between (should be >= 500,000 and <= 2,000,000 observations.): 574,748\n",
    "- OHCO Structure (as delimitted column names): `script_id`,\t`page_num`,\t`sent_num`,\t`token_num`\t\n",
    "- Columns (as delimitted column names, including `token_str`, `term_str`, `pos`, and `pos_group`): `script_id`,\t`page_num`,\t`sent_num`,\t`token_num`, `pos_tuple`, `pos`, `token_str`, `term_str`, `pos_group`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae3214e-e6dd-42d6-842f-555d0058986e",
   "metadata": {},
   "source": [
    "## VOCAB (2)\n",
    "\n",
    "The unique word types (terms) in the corpus.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/7kax43z51qztkz57pb31wxtceby6tyn8\n",
    "- GitHub URL for notebook used to create: https://github.com/hmf9kx/DS5001FinalProject/blob/main/FinalProjectCode.ipynb\n",
    "- Delimitter: `|`\n",
    "- Number of observations: 22217\n",
    "- Columns (as delimitted names, including `n`, `p`', `i`, `dfidf`, `porter_stem`, `max_pos` and `max_pos_group`, `stop`): `term_str`, `n`, `p`,\t`i`, `n_chars`,\t`max_pos_group`, `max_pos`,\t`p_stem`, `stop`, `DFIDF`,\n",
    "- Note: Your VOCAB may contain ngrams. If so, add a feature for `ngram_length`.\n",
    "- List the top 20 significant words in the corpus by DFIDF.\n",
    "\n",
    "'down', 'not', 'me', 'into', 'have', 'there', 'back', 'be', 'they', 'just', 're', 'looks', 'like', 'one', 'no', 'all', 'then', 'she', 'him', 'but'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40dabdc-baae-4408-95bc-2f735824d59b",
   "metadata": {},
   "source": [
    "# Derived Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f2ef9c-1cb5-41e8-a5ee-1e37428b4539",
   "metadata": {},
   "source": [
    "## BOW (3)\n",
    "\n",
    "A bag-of-words representation of the CORPUS.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/zcaw8hh2wyvpunlp5495hiq0mw5xq4wi\n",
    "- GitHub URL for notebook used to create: https://github.com/hmf9kx/DS5001FinalProject/blob/main/FinalProjectCode.ipynb\n",
    "- Delimitter: `|`\n",
    "- Bag (expressed in terms of OHCO levels): `script_id`, `page_num`\n",
    "- Number of observations: 321,665\n",
    "- Columns (as delimitted names, including `n`, `tfidf`): `script_id`, `page_num`, `term_str`, `n`, `tfidf`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29890d2f-bf96-43ad-8d08-792393830163",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DTM (3)\n",
    "\n",
    "A represenation of the BOW as a sparse count matrix.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/jqvohm9dec8il323fhf59adi76mveigj\n",
    "- UVA Box URL of BOW used to generate (if applicable): https://virginia.app.box.com/file/1516177645215\n",
    "- GitHub URL for notebook used to create: https://github.com/hmf9kx/DS5001FinalProject/blob/main/FinalProjectCode.ipynb\n",
    "- Delimitter: `|`\n",
    "- Bag (expressed in terms of OHCO levels): `script_id`, `page_num`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4b4774-7c76-401d-a9de-2704f28a0821",
   "metadata": {},
   "source": [
    "## TFIDF (3)\n",
    "\n",
    "A Document-Term matrix with TFIDF values.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/x4qcc5d4wymg9avq84bksv0w3e5a74o6\n",
    "- UVA Box URL of DTM or BOW used to create: https://virginia.box.com/s/zcaw8hh2wyvpunlp5495hiq0mw5xq4wi\n",
    "- GitHub URL for notebook used to create: https://github.com/hmf9kx/DS5001FinalProject/blob/main/FinalProjectCode.ipynb\n",
    "- Delimitter: `|`\n",
    "- Description of TFIDIF formula ($\\LaTeX$ OK): I computed the maximum TFIDF: $$\\Large w_{t,d} = \\frac{f_{t,d}}{max_{\\{t^\\prime\\in d\\}} f_{t^\\prime,d}} * log(\\frac{N}{f_d})$$ where $f_{t,d}$ is the count of term $t$ in document $d$ and where $f_d$ is the count of documents containing term $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd34f5ca-5361-4701-b9dd-9da66859b40b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reduced and Normalized TFIDF_L2 (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c548dd2-f692-4365-936c-39c84df79b90",
   "metadata": {
    "tags": []
   },
   "source": [
    "A Document-Term matrix with L2 normalized TFIDF values.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/7hwtgoj0ghylcbnqp2f2r9mre3w5qbgg\n",
    "- UVA Box URL of source TFIDF table: https://virginia.box.com/s/x4qcc5d4wymg9avq84bksv0w3e5a74o6\n",
    "- GitHub URL for notebook used to create: https://github.com/hmf9kx/DS5001FinalProject/blob/main/FinalProjectCode.ipynb\n",
    "- Delimitter: `|`\n",
    "- Number of features (i.e. significant words): 1000\n",
    "- Principle of significant word selection: Top 1000 signficant words based on DFIDF value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50da94-af36-4e8d-b1a7-24dbcf431880",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df79264-dd93-4199-be38-db31579b7ce8",
   "metadata": {},
   "source": [
    "## PCA Components (4)\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/nd3gj9imka395yng5nfyxzm0dfau31ia\n",
    "- UVA Box URL of the source TFIDF_L2 table: https://virginia.box.com/s/7hwtgoj0ghylcbnqp2f2r9mre3w5qbgg\n",
    "- GitHub URL for notebook used to create: https://github.com/hmf9kx/DS5001FinalProject/blob/main/FinalProjectCode.ipynb\n",
    "- Delimitter: `|`\n",
    "- Number of components: 10\n",
    "- Library used to generate: Adapted from the in-class example which was created from scratch, with help from the scipy library\n",
    "- Top 5 positive terms for first component: mrs, door, room, int, dr\n",
    "- Top 5 negative terms for second component: mrs, mother, truck, group, bug\n",
    "\n",
    "Notes: While the terms are not particularly interesting, it is pretty clear that these would appear in many of the scripts. It is worth noting that terms such as `mrs`, `dr`, and `mother` likely dominate the dialogues in several scripts, whereas `door`, `room`, and `int` (i.e. interior) would be used in establishing scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73adc882-cbce-4d24-9923-5d36ac609f43",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PCA DCM (4)\n",
    "\n",
    "The document-component matrix generated.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/xphxze32r2uhuiawyobc7z1hymqmtyeb\n",
    "- GitHub URL for notebook used to create: https://github.com/hmf9kx/DS5001FinalProject/blob/main/FinalProjectCode.ipynb\n",
    "- Delimitter: `|`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fd2a4a-7f2f-4259-a5c4-063168cb1b14",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PCA Loadings (4)\n",
    "\n",
    "The component-term matrix generated.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/6qiqabfqofzdukkzm09rngcexrhd1auo\n",
    "- GitHub URL for notebook used to create: https://github.com/hmf9kx/DS5001FinalProject/blob/main/FinalProjectCode.ipynb\n",
    "- Delimitter: `|`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fff42f-6665-4941-ba3d-034627dc0124",
   "metadata": {},
   "source": [
    "## PCA Visualization 1 (4)\n",
    "\n",
    "Include a scatterplot of documents in the space created by the first two components.\n",
    "\n",
    "Color the points based on a metadata feature associated with the documents.\n",
    "\n",
    "Also include a scatterplot of the loadings for the same two components. (This does not need a feature mapped onto color.)\n",
    "\n",
    "![](./Images/pc1.png)\n",
    "\n",
    "![](./Images/l1.png)\n",
    "\n",
    "Briefly describe the nature of the polarity you see in the first component:\n",
    "\n",
    "The polarity of the first two components in conjunction seem to represent a polarity between interior dialogue and exterior action.  Values high along the first component include `mrs`, `dr`, and `mother` (used for dialogue) as well as `room`, `int` (interior), and `kitchen`. Values at the other end include `truck`, `road`, `gun`, `fire`, and `horse`. It is worth noting how much of an outlier `mrs` is. I suspect that is a term used very frequently in dialogue across many of the scripts. It would likely have been joined by other names/pronouns had they not been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb54565-7669-4a2f-90b2-a4c283277c02",
   "metadata": {},
   "source": [
    "## PCA Visualization 2 (4)\n",
    "\n",
    "Include a scatterplot of documents in the space created by the second two components.\n",
    "\n",
    "Color the points based on a metadata feature associated with the documents.\n",
    "\n",
    "Also include a scatterplot of the loadings for the same two components. (This does not need a feature mapped onto color.)\n",
    "\n",
    "![](./Images/pc2.png)\n",
    "\n",
    "![](./Images/l2.png)\n",
    "\n",
    "Briefly describe the nature of the polarity you see in the second component:\n",
    "\n",
    "The second pairing of components is harder to interpret. Without too much speculation, I think some conclusions can be drawn from the box and whisker plots. It is interesting how comedy and romance are shifted left along the principal component whereas horror and sci-fi are shifted right, with western in the middle. I think it is fair to say that romance and comedy are the two most similar genres, especially since some of the films selected could be classified as rom-coms. They are contrasted with sci-fi and horror, which have the potential to contain the heaviest themes. So it is possible this component represents the theming or sentiment of the different genres. Whether or not this is an accurate interpretation, this is an idea that I will explore further later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ee23b2-25d1-4226-bf31-1607e5ed4677",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LDA TOPIC (4)\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/9yuzvkm2700rmgsd8rwgjw6esfncx9fe\n",
    "- UVA Box URL of count matrix used to create: https://virginia.box.com/s/jqvohm9dec8il323fhf59adi76mveigj\n",
    "- GitHub URL for notebook used to create: https://github.com/hmf9kx/DS5001FinalProject/blob/main/FinalProjectCode.ipynb\n",
    "- Delimitter: `|`\n",
    "- Libary used to compute: sklearn\n",
    "- A description of any filtering, e.g. POS (Nouns and Verbs only): Nouns only\n",
    "- Number of components: 20\n",
    "- Any other parameters used:\n",
    "- Top 5 words and best-guess labels for top five topics by mean document weight:\n",
    "  - T00: door room dr face hand (hospital room)\n",
    "  - T01: ext way door gun beat (exterior conflict)\n",
    "  - T02: car ext man int street\t(street scene)\n",
    "  - T03: door int room night bed (nighttime bedroom scene)\n",
    "  - T04: room int table people way (interior discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a518d520-4a5c-48fa-836d-f8ea3e3c2f06",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LDA THETA (4)\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/qmugzxl8nbvundyp27m97vy2ar0n3wu1\n",
    "- GitHub URL for notebook used to create: https://github.com/hmf9kx/DS5001FinalProject/blob/main/FinalProjectCode.ipynb\n",
    "- Delimitter: `|`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8808b30-64f4-4249-95d5-d7c0925ce432",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LDA PHI (4)\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/sxxzb2dh45q1qxbc20vlqqqm35sky6v7\n",
    "- GitHub URL for notebook used to create: https://github.com/hmf9kx/DS5001FinalProject/blob/main/FinalProjectCode.ipynb\n",
    "- Delimitter: `|`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e404bf-8a2a-4eb4-ba89-0c708c8f359d",
   "metadata": {},
   "source": [
    "## LDA + PCA Visualization (4)\n",
    "\n",
    "Apply PCA to the PHI table and plot the topics in the space opened by the first two components.\n",
    "\n",
    "Size the points based on the mean document weight of each topic (using the THETA table).\n",
    "\n",
    "Color the points basd on a metadata feature from the LIB table.\n",
    "\n",
    "Provide a brief interpretation of what you see.\n",
    "\n",
    "![](./Images/lda.png)\n",
    "\n",
    "We see that genres seem to be clustered fairly well in this space. It is interesting that most topics are on the left side of the graph, indicating similar values for the first principal component. The main exception to this rule is topic 15, which is the 'street scene' topic we saw earlier. I am not sure what about this topic differentiates it so clearly from the others (at least along the first principal component)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e1f327-a386-476a-8d94-2ab7a63afa7a",
   "metadata": {},
   "source": [
    "## Sentiment VOCAB_SENT (4)\n",
    "\n",
    "Sentiment values associated with a subset of the VOCAB from a curated sentiment lexicon.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/wbvhueqcpli42xhswq11n2nxawgzeygm\n",
    "- UVA Box URL for source lexicon: https://virginia.box.com/s/rs2gc1h48m94u93qz3vv6u4ha6ilnypt\n",
    "- GitHub URL for notebook used to create: https://github.com/hmf9kx/DS5001FinalProject/blob/main/FinalProjectCode.ipynb\n",
    "- Delimitter: `|`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a9d67-1560-4be9-b82a-b99a60b5c93e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sentiment BOW_SENT (4)\n",
    "\n",
    "Sentiment values from VOCAB_SENT mapped onto BOW.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/jenp8v4nhu7shu68bv4h1051aipobqsi\n",
    "- GitHub URL for notebook used to create: https://github.com/hmf9kx/DS5001FinalProject/blob/main/FinalProjectCode.ipynb\n",
    "- Delimitter: `|`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ee6837-b12e-453d-96c1-59eaa4b28883",
   "metadata": {},
   "source": [
    "## Sentiment DOC_SENT (4)\n",
    "\n",
    "Computed sentiment per bag computed from BOW_SENT.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/u2aqjbv41del4is2l4smdloyzt17tm4k\n",
    "- GitHub URL for notebook used to create: https://github.com/hmf9kx/DS5001FinalProject/blob/main/FinalProjectCode.ipynb\n",
    "- Delimitter: `|`\n",
    "- Document bag expressed in terms of OHCO levels: `script_id`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4cba13-e60a-4940-a06d-02479f002c3c",
   "metadata": {},
   "source": [
    "## Sentiment Plot (4)\n",
    "\n",
    "Plot sentiment over some metric space, such as time.\n",
    "\n",
    "If you don't have a metric metadata features, plot sentiment over a feature of your choice.\n",
    "\n",
    "You may use a bar chart or a line graph.\n",
    "\n",
    "![](./Images/fault_sentiment.png)\n",
    "\n",
    "This plot shows the sentiment over time (slightly smoothed as pages were quite granular as a bag) for the movie \"The Fault in Our Stars\". I chose to visualize this movie as it definitely has its ups and downs, which I think were captured quite accurately. The first two peaks around pages 20 and 40 show the two love interests agreeing to read each other's favorite books and their first kiss, respectively. The first large dip represents an argument, the next peak a successful dinner, and the biggest dip the moment where the two main characters realize that their idol is not who they have made him out to be. The last section contains mainy highs and lows including the death of one of the main characters, the redemption of the idol, and an overall bittersweet ending. Overall, I thought this visual did a good job at summarizing the sentiment of the novel over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d2316-317b-4d95-a804-aff98242e411",
   "metadata": {},
   "source": [
    "## VOCAB_W2V (4)\n",
    "\n",
    "A table of word2vec features associated with terms in the VOCAB table.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/fbyii9lf9qqguo117lwz4x6aeh1vpwjs\n",
    "- GitHub URL for notebook used to create: https://github.com/hmf9kx/DS5001FinalProject/blob/main/FinalProjectCode.ipynb\n",
    "- Delimitter: `\\`\n",
    "- Document bag expressed in terms of OHCO levels: `script_id`, `page_num`\n",
    "- Number of features generated: 246\n",
    "- The library used to generate the embeddings: gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c1974-047b-4285-9f4d-7f3314f39542",
   "metadata": {},
   "source": [
    "## Word2vec tSNE Plot (4)\n",
    "\n",
    "Plot word embedding featues in two-dimensions using t-SNE.\n",
    "\n",
    "Describe a cluster in the plot that captures your attention.\n",
    "\n",
    "![](./Images/tsne.png)\n",
    "\n",
    "![](./Images/tsne_zoom.png)\n",
    "\n",
    "I thought this cluster was interesting, it seems centered around parts of the body (`arms`, `fingers`, `body`), which then extends to corresponding actions (`holds`, `lifts`, `throws`) and items (`jacket`, `gun`, `pocket`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75878341-7fe8-4e22-b908-36029f9818e8",
   "metadata": {},
   "source": [
    "# Riffs\n",
    "\n",
    "Provde at least three visualizations that combine the preceding model data in interesting ways.\n",
    "\n",
    "These should provide insight into how features in the LIB table are related. \n",
    "\n",
    "The nature of this relationship is left open to you -- it may be correlation, or mutual information, or something less well defined. \n",
    "\n",
    "In doing so, consider the following visualization types:\n",
    "\n",
    "- Hierarchical cluster diagrams\n",
    "- Heatmaps\n",
    "- Scatter plots\n",
    "- KDE plots\n",
    "- Dispersion plots\n",
    "- t-SNE plots\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c62acf1-6bb0-45d0-aed2-863b285f8cad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Riff 1 (5)\n",
    "\n",
    "![](./Images/genre_sentiment.png)\n",
    "\n",
    "I was interested in extending the sentiment plot over time to other scripts and genres. To look at the aggregate, I normalized the length of each scripts (because of varying page counts) and averaged sentiment by genre over a rolling window. This lets us compare the average sentiment by genre as the story progresses. I was happy to see that the comedy and romance genres float along the top of the graph where as horror tends to stay along the bottom. There is obviously a lot of variation from movie to movie, but there are some larger-scale patterns that emerge. I thought it was interesting that both western and horror movies had a large rise in sentiment around 80% of the way through. Persumably this is some catharsis after the climax of the final conflict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2155a072-02b3-4aa8-b9f1-e43a59e9a85d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Riff 2 (5)\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"./Images/cluster.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "I decided to see how well clustering similarity would perform on distinguishing the genres. This graph was generated using cosine similarity and Ward linkage. The clusters came out quite well, especially for comedy and romance. It is interesting to see the same trend from the PCA box and whisker plots represented here, with romance/comedy on one end, horror/sci-fi on the other, and western in the middle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5067c59b-8983-4acc-972a-1ecd852ded57",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Riff 3 (5)\n",
    "\n",
    "![](./Images/heatmap.png)\n",
    "\n",
    "- 100s- Western\n",
    "- 200s- Sci-Fi\n",
    "- 300s- Horror\n",
    "- 400s- Comedy\n",
    "- 500s- Romance\n",
    "\n",
    "\n",
    "Finally, I wanted to further examine this idea of the similarity between romance and comedy. I created a heatmap representing the correlation (Kendall rank correlation) between each script. Unsurprisingly, we do see genre-defining boxes around the main diagonal, with the darkest being around the comedies and romances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e25c6e-2624-4899-829e-e7d60c878685",
   "metadata": {},
   "source": [
    "# Interpretation (4)\n",
    "\n",
    "Describe something interesting about your corpus that you discovered during the process of completing this assignment.\n",
    "\n",
    "At a minumum, use 250 words, but you may use more. You may also add images if you'd like.\n",
    "\n",
    "\n",
    "I think the most interesting thing I learned from this corpus was the difference in analyzing movie scripts as opposed to the novels we have practiced with in class. The scripts had a hidden internal structure where they contained both scenes and dialogues. The source data also had a corpus that parsed scenes and dialogues separately, but I decided to work with the raw data since a movie is much more than either of these two constituent parts. However, this did mean that words such as interior, exterior, voiceover, etc. which are used conventionally to establish the structure of a scene or the nature of a dialogue appeared time and time again as significant in the corpus. While it is certainly true that these words are important to the writing of a script, I felt that they were more like metadata which served to obfuscate some of my results. I think my PCA suffered the most from this, with some of the main components seeming to represent more surface-level structural dichotomies (e.g. is a scene inside or outside, is it more of a dialogue or an action, etc.) as opposed to the deeper literary patterns I would have liked to discover. With that being said, having genre as a feature led to some interesting analysis. It was neat to see how certain genres (most notably romance and comedy) followed similar distributions across different models. I also felt like the sentiment analysis I did worked quite well for the scripts, potentially because writers must give actors detailed descriptions of their character's emotions. Overall, I do think that there was a lot of value to this analysis and there is still potential for many more results. If I had more time, I would be interested in running separate analyses on the scenes and the dialogues. I could also try and better parse through the vocabulary and remove some of the keywords I mentioned above. Finally, there is much more metadata that I could add to the LIB table. I would be interested to see if patterns in release data, runtime, or even director age/gender would appear in the analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
